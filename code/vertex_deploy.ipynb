{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a58281c",
   "metadata": {},
   "source": [
    "# Setup use case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c7d622",
   "metadata": {},
   "source": [
    "### Enable  the APIs if they are not enabled "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce260c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''!gcloud services enable compute.googleapis.com         \\\n",
    "                       containerregistry.googleapis.com  \\\n",
    "                       aiplatform.googleapis.com  \\\n",
    "                       cloudbuild.googleapis.com \\\n",
    "                       cloudfunctions.googleapis.com\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a829d243",
   "metadata": {},
   "source": [
    "## Uncomment all needed cells if you need to install missing packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689753be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install ai platform and kfp\n",
    "#!pip3 install {USER_FLAG} google-cloud-aiplatform==1.3.0 --upgrade\n",
    "#!pip3 install {USER_FLAG} kfp --upgrade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa761c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install google_cloud_pipeline_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba102fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!gcloud auth login if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ceae6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart the kernel\n",
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c08242",
   "metadata": {},
   "source": [
    "#### Set up the global variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4377cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH=%env PATH\n",
    "%env PATH={PATH}:/home/jupyter/.local/bin\n",
    "\n",
    "CLIENT_PROJECT_NAME = 'test' \n",
    "REGION=\"europe-west1\"\n",
    "\n",
    "# Get projet name\n",
    "shell_output=!gcloud config get-value project 2> /dev/null\n",
    "PROJECT_ID=shell_output[0]\n",
    "\n",
    "# Set bucket name\n",
    "BUCKET_NAME=\"gs://\"+PROJECT_ID+\"-bucket-\" + CLIENT_PROJECT_NAME\n",
    "\n",
    "# Bucket data\n",
    "BUCKET_DATA  = BUCKET_NAME + \"/data/\"\n",
    "\n",
    "# Create bucket\n",
    "PIPELINE_ROOT = f\"{BUCKET_NAME}/pipeline_root/\"\n",
    "USER_FLAG = \"--user\"\n",
    "\n",
    "# Create bucket\n",
    "!gsutil mb -c standard -l $REGION $BUCKET_NAME\n",
    "\n",
    "\n",
    "# Model Target \n",
    "TARGET= \"target\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c4c16c",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    " * Artifact,\n",
    " * Dataset,\n",
    " * Input,\n",
    " * Model,\n",
    " * Output,\n",
    " * Metrics,\n",
    " * ClassificationMetrics\n",
    " * InputPath\n",
    " * OutputPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95845cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "import typing\n",
    "from kfp.v2 import dsl\n",
    "from kfp.v2.dsl import (Artifact,\n",
    "                        Dataset,\n",
    "                        Input,\n",
    "                        Model,\n",
    "                        Output,\n",
    "                        Metrics,\n",
    "                        ClassificationMetrics,\n",
    "                        component, \n",
    "                        OutputPath, \n",
    "                        InputPath)\n",
    "\n",
    "from kfp.v2 import compiler\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459d2b87",
   "metadata": {},
   "source": [
    "### Create pipeline\n",
    "\n",
    "We create 4 components:  \n",
    "- Load data   \n",
    "- Train a  model\n",
    "- Evaluate the model \n",
    "- Deploy the model\n",
    "\n",
    "The components have dependencies on `pandas`, `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bd2d50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://../data/test.csv [Content-Type=text/csv]...\n",
      "Copying file://../data/train.csv [Content-Type=text/csv]...                     \n",
      "/ [2 files][ 16.3 KiB/ 16.3 KiB]                                                \n",
      "Operation completed over 2 objects/16.3 KiB.                                     \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c0</th>\n",
       "      <th>c1</th>\n",
       "      <th>c2</th>\n",
       "      <th>c3</th>\n",
       "      <th>c4</th>\n",
       "      <th>c5</th>\n",
       "      <th>c6</th>\n",
       "      <th>c7</th>\n",
       "      <th>c8</th>\n",
       "      <th>c9</th>\n",
       "      <th>c10</th>\n",
       "      <th>c11</th>\n",
       "      <th>c12</th>\n",
       "      <th>c13</th>\n",
       "      <th>c14</th>\n",
       "      <th>c15</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>73960.044714</td>\n",
       "      <td>84403.422147</td>\n",
       "      <td>81100.834420</td>\n",
       "      <td>78754.613864</td>\n",
       "      <td>63716.529225</td>\n",
       "      <td>71794.028431</td>\n",
       "      <td>81012.621631</td>\n",
       "      <td>65109.664137</td>\n",
       "      <td>81444.184151</td>\n",
       "      <td>79405.859619</td>\n",
       "      <td>59386.103715</td>\n",
       "      <td>56478.892203</td>\n",
       "      <td>76611.413924</td>\n",
       "      <td>88685.805901</td>\n",
       "      <td>70858.600148</td>\n",
       "      <td>99536.372888</td>\n",
       "      <td>75156.105188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>87196.743619</td>\n",
       "      <td>97688.792258</td>\n",
       "      <td>93162.659601</td>\n",
       "      <td>106943.277331</td>\n",
       "      <td>63716.529225</td>\n",
       "      <td>106327.598799</td>\n",
       "      <td>97231.352482</td>\n",
       "      <td>65440.025158</td>\n",
       "      <td>109821.459689</td>\n",
       "      <td>89395.444452</td>\n",
       "      <td>88829.850983</td>\n",
       "      <td>80106.612534</td>\n",
       "      <td>82711.507077</td>\n",
       "      <td>84128.484305</td>\n",
       "      <td>82292.083678</td>\n",
       "      <td>79086.943250</td>\n",
       "      <td>75995.843234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84406.173121</td>\n",
       "      <td>80768.608520</td>\n",
       "      <td>82488.153945</td>\n",
       "      <td>64577.221383</td>\n",
       "      <td>63716.529225</td>\n",
       "      <td>107533.416515</td>\n",
       "      <td>97020.719614</td>\n",
       "      <td>63599.442325</td>\n",
       "      <td>81444.184151</td>\n",
       "      <td>83644.594356</td>\n",
       "      <td>66897.749926</td>\n",
       "      <td>71873.957366</td>\n",
       "      <td>74250.819737</td>\n",
       "      <td>81926.632298</td>\n",
       "      <td>67693.694533</td>\n",
       "      <td>81981.011610</td>\n",
       "      <td>78760.212392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>77106.608256</td>\n",
       "      <td>52401.890830</td>\n",
       "      <td>57943.848129</td>\n",
       "      <td>56539.654989</td>\n",
       "      <td>63716.529225</td>\n",
       "      <td>61817.883698</td>\n",
       "      <td>85056.772701</td>\n",
       "      <td>72566.384334</td>\n",
       "      <td>68545.422543</td>\n",
       "      <td>68364.314626</td>\n",
       "      <td>91545.727481</td>\n",
       "      <td>59524.974615</td>\n",
       "      <td>62271.939137</td>\n",
       "      <td>73743.935769</td>\n",
       "      <td>69077.030762</td>\n",
       "      <td>60483.006478</td>\n",
       "      <td>80101.853092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>88476.318181</td>\n",
       "      <td>63491.398777</td>\n",
       "      <td>72973.708834</td>\n",
       "      <td>61949.179344</td>\n",
       "      <td>63716.529225</td>\n",
       "      <td>61529.486063</td>\n",
       "      <td>64477.941478</td>\n",
       "      <td>77521.799655</td>\n",
       "      <td>56936.537096</td>\n",
       "      <td>83477.206477</td>\n",
       "      <td>63328.371431</td>\n",
       "      <td>63641.302199</td>\n",
       "      <td>74148.678642</td>\n",
       "      <td>56405.631360</td>\n",
       "      <td>89648.917260</td>\n",
       "      <td>57825.143545</td>\n",
       "      <td>80202.857141</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             c0            c1            c2             c3            c4  \\\n",
       "0  73960.044714  84403.422147  81100.834420   78754.613864  63716.529225   \n",
       "1  87196.743619  97688.792258  93162.659601  106943.277331  63716.529225   \n",
       "2  84406.173121  80768.608520  82488.153945   64577.221383  63716.529225   \n",
       "3  77106.608256  52401.890830  57943.848129   56539.654989  63716.529225   \n",
       "4  88476.318181  63491.398777  72973.708834   61949.179344  63716.529225   \n",
       "\n",
       "              c5            c6            c7             c8            c9  \\\n",
       "0   71794.028431  81012.621631  65109.664137   81444.184151  79405.859619   \n",
       "1  106327.598799  97231.352482  65440.025158  109821.459689  89395.444452   \n",
       "2  107533.416515  97020.719614  63599.442325   81444.184151  83644.594356   \n",
       "3   61817.883698  85056.772701  72566.384334   68545.422543  68364.314626   \n",
       "4   61529.486063  64477.941478  77521.799655   56936.537096  83477.206477   \n",
       "\n",
       "            c10           c11           c12           c13           c14  \\\n",
       "0  59386.103715  56478.892203  76611.413924  88685.805901  70858.600148   \n",
       "1  88829.850983  80106.612534  82711.507077  84128.484305  82292.083678   \n",
       "2  66897.749926  71873.957366  74250.819737  81926.632298  67693.694533   \n",
       "3  91545.727481  59524.974615  62271.939137  73743.935769  69077.030762   \n",
       "4  63328.371431  63641.302199  74148.678642  56405.631360  89648.917260   \n",
       "\n",
       "            c15        target  \n",
       "0  99536.372888  75156.105188  \n",
       "1  79086.943250  75995.843234  \n",
       "2  81981.011610  78760.212392  \n",
       "3  60483.006478  80101.853092  \n",
       "4  57825.143545  80202.857141  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import data to GCS \n",
    "!gsutil cp ../data/*.csv $BUCKET_DATA\n",
    "df_raw_data = pd.read_csv(\"../data/train.csv\", delimiter=\";\")\n",
    "df_raw_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fa24c0",
   "metadata": {},
   "source": [
    "### Read the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51b051c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"pandas\", \"pyarrow\", \"scikit-learn==1.0.0\", \"gcsfs\"],\n",
    "    base_image=\"python:3.9\",\n",
    "    output_component_file=\"get_test_train_data.yaml\"\n",
    ")\n",
    "\n",
    "def get_data(\n",
    "    url: str,\n",
    "    dataset_train: Output[Dataset],\n",
    "    dataset_test: Output[Dataset]\n",
    "):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "   \n",
    "    train, test = pd.read_csv(url + 'train.csv', sep=';'), pd.read_csv(url + 'test.csv', sep=';')\n",
    "    train.to_csv(dataset_train.path + \".csv\" , sep=';',index=False, encoding='utf-8-sig')\n",
    "    test.to_csv(dataset_test.path + \".csv\" , sep=';',index=False, encoding='utf-8-sig')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de33e87f",
   "metadata": {},
   "source": [
    "#### Train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a50fc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install = [\"pandas\", \"pyarrow\", \"scikit-learn==1.0.0\",\"gcsfs\"], \n",
    "    base_image=\"python:3.9\",\n",
    "    output_component_file=\"train_model.yaml\"\n",
    ")\n",
    "def model_training(\n",
    "    target: str,\n",
    "    dataset:  Input[Dataset],\n",
    "    model: Output[Model], \n",
    "):\n",
    "    \n",
    "    from sklearn.linear_model import Ridge\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "\n",
    "    data = pd.read_csv(dataset.path+\".csv\", sep=';')\n",
    "    print(data.columns)\n",
    "    model_reg = Ridge(alpha= 102 ,positive = True)\n",
    "    model_reg.fit(\n",
    "        data.drop(columns=target),\n",
    "        data[target]\n",
    "    )\n",
    "    \n",
    "    model.metadata[\"framework\"] = \"Ridge\"\n",
    "    file_name = model.path + f\".pkl\"\n",
    "    with open(file_name, 'wb') as file:  \n",
    "        pickle.dump(model_reg, file)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdec3aea",
   "metadata": {},
   "source": [
    "#### Evaluate the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1b304bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install = [\"pandas\", \"pyarrow\", \"scikit-learn==1.0.0\", \"gcsfs\"], \n",
    "    base_image=\"python:3.9\",\n",
    "    output_component_file=\"evaluate_model.yaml\"\n",
    ")\n",
    "def model_evaluation(\n",
    "    target : str,\n",
    "    test_set:  Input[Dataset],\n",
    "    model: Input[Model],\n",
    "    thresholds_dict_str: str,\n",
    "    kpi: Output[Metrics]\n",
    ") -> NamedTuple(\"output\", [(\"deploy\", str)]):\n",
    "\n",
    "    #from sklearn.linear_model import Ridge\n",
    "    import pandas as pd\n",
    "    import logging \n",
    "    import pickle\n",
    "    from sklearn.metrics import r2_score\n",
    "    import json\n",
    "    import typing\n",
    "\n",
    "    \n",
    "    def threshold_check(val1, val2):\n",
    "        cond = \"false\"\n",
    "        if val1 >= val2 :\n",
    "            cond = \"true\"\n",
    "        return cond\n",
    "\n",
    "    data = pd.read_csv(test_set.path+\".csv\", sep = ';')\n",
    "    file_name = model.path + \".pkl\"\n",
    "    with open(file_name, 'rb') as file:  \n",
    "        model_reg = pickle.load(file)\n",
    "    \n",
    "    y_test = data.drop(columns=target)\n",
    "    y_target=data[target]\n",
    "    y_pred = model_reg.predict(y_test)\n",
    "       \n",
    "    R2 = r2_score(y_target, y_pred)\n",
    "    thresholds_dict = json.loads(thresholds_dict_str)\n",
    "    model.metadata[\"R2\"] = float(R2)\n",
    "    kpi.log_metric(\"R2\", float(R2))\n",
    "    deploy = threshold_check(float(R2), int(thresholds_dict['R2']))\n",
    "    return (deploy,)\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139b5ad2",
   "metadata": {},
   "source": [
    "### Deploy model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ae07652",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"google-cloud-aiplatform\", \"pandas\", \"pyarrow\", \"scikit-learn==1.0.0\", \"gcsfs\",  \"kfp\"],\n",
    "    base_image=\"python:3.9\",\n",
    "    output_component_file=\"pipeline_coponents.yml\"\n",
    ")\n",
    "def deploy_model(\n",
    "    model: Input[Model],\n",
    "    project: str,\n",
    "    region: str,\n",
    "    serving_container_image_uri : str, \n",
    "    vertex_endpoint: Output[Artifact],\n",
    "    vertex_model: Output[Model]\n",
    "):\n",
    "    from google.cloud import aiplatform\n",
    "    aiplatform.init(project=project, location=region)\n",
    "\n",
    "    DISPLAY_NAME  = \"test\"\n",
    "    MODEL_NAME = \"test-ridge\"\n",
    "    ENDPOINT_NAME = \"test_endpoint\"\n",
    "    \n",
    "    def create_endpoint():\n",
    "        endpoints = aiplatform.Endpoint.list(\n",
    "        filter='display_name=\"{}\"'.format(ENDPOINT_NAME),\n",
    "        order_by='create_time desc',\n",
    "        project=project, \n",
    "        location=region,\n",
    "        )\n",
    "        if len(endpoints) > 0:\n",
    "            endpoint = endpoints[0]  # most recently created\n",
    "        else:\n",
    "            endpoint = aiplatform.Endpoint.create(\n",
    "            display_name=ENDPOINT_NAME, project=project, location=region\n",
    "        )\n",
    "    endpoint = create_endpoint()   \n",
    "    \n",
    "    \n",
    "    #Import a model programmatically\n",
    "    model_upload = aiplatform.Model.upload(\n",
    "        display_name = DISPLAY_NAME, \n",
    "        artifact_uri = model.uri[:-5], #.replace(\"model\", \"\"),\n",
    "        serving_container_image_uri =  serving_container_image_uri,\n",
    "        serving_container_health_route=f\"/v1/models/{MODEL_NAME}\",\n",
    "        serving_container_predict_route=f\"/v1/models/{MODEL_NAME}:predict\",\n",
    "        serving_container_environment_variables={\n",
    "        \"MODEL_NAME\": MODEL_NAME,\n",
    "    },       \n",
    "    )\n",
    "    model_deploy = model_upload.deploy(\n",
    "        machine_type=\"n1-standard-4\", \n",
    "        endpoint=endpoint,\n",
    "        traffic_split={\"0\": 100},\n",
    "        deployed_model_display_name=DISPLAY_NAME,\n",
    "    )\n",
    "\n",
    "    # Save data to the output params\n",
    "    vertex_model.uri = model_deploy.resource_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7902a0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP =datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "DISPLAY_NAME = 'pipeline-test-job{}'.format(TIMESTAMP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98bd8c3",
   "metadata": {},
   "source": [
    "#### Create the Pipeline\n",
    "\n",
    "Once you have created all the needed components define the pipeline and then compile it into a `.json` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3d5f227",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    # Default pipeline root. You can override it when submitting the pipeline.\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    # A name for the pipeline. Use to determine the pipeline Context.\n",
    "    name=\"pipeline-test\",\n",
    "    \n",
    ")\n",
    "def pipeline(\n",
    "    url: str = BUCKET_NAME + \"/data/\",\n",
    "    target : str = TARGET,\n",
    "    project: str = PROJECT_ID,\n",
    "    region: str = REGION, \n",
    "    display_name: str = DISPLAY_NAME,\n",
    "    api_endpoint: str = REGION+\"-aiplatform.googleapis.com\",\n",
    "    thresholds_dict_str: str = '{\"R2\":0.45}',\n",
    "    serving_container_image_uri: str = \"europe-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.0-24:latest\"\n",
    "    ):\n",
    "    \n",
    "    data_op = get_data(url)\n",
    "    \n",
    "    train_model_op = model_training(target, data_op.outputs[\"dataset_train\"])\n",
    "    \n",
    "    model_evaluation_op = model_evaluation(\n",
    "        target,\n",
    "        test_set=data_op.outputs[\"dataset_test\"],\n",
    "        model=train_model_op.outputs[\"model\"],\n",
    "        thresholds_dict_str = thresholds_dict_str, # I deploy the model only if the model performance is above the threshold\n",
    "    )\n",
    "    \n",
    "    with dsl.Condition(\n",
    "        model_evaluation_op.outputs[\"deploy\"]==\"true\",\n",
    "        name=\"deploy-model\",\n",
    "    ):\n",
    "           \n",
    "        deploy_model_op = deploy_model(\n",
    "        model=train_model_op.outputs['model'],\n",
    "        project=project,\n",
    "        region=region, \n",
    "        serving_container_image_uri = serving_container_image_uri,\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba00f47",
   "metadata": {},
   "source": [
    "### Compile and run the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d68ec6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.7/site-packages/kfp/v2/compiler/compiler.py:1266: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  category=FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "compiler.Compiler().compile(pipeline_func=pipeline,package_path='ml_pipeline.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68be6e6",
   "metadata": {},
   "source": [
    "The pipeline compilation generates the **ml_pipeline.json** job spec file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ec004f-3476-4cf0-83f5-cd71330584c4",
   "metadata": {},
   "source": [
    "### Create a run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b24bac81",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_pipeline = pipeline_jobs.PipelineJob(\n",
    "    display_name=\"pipeline-test\",\n",
    "    template_path=\"ml_pipeline.json\",\n",
    "    enable_caching=False,\n",
    "    location=REGION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f7bd3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/716383847597/locations/europe-west1/pipelineJobs/pipeline-test-20220309213045\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/716383847597/locations/europe-west1/pipelineJobs/pipeline-test-20220309213045')\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/europe-west1/pipelines/runs/pipeline-test-20220309213045?project=716383847597\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/716383847597/locations/europe-west1/pipelineJobs/pipeline-test-20220309213045 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/716383847597/locations/europe-west1/pipelineJobs/pipeline-test-20220309213045 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/716383847597/locations/europe-west1/pipelineJobs/pipeline-test-20220309213045 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/716383847597/locations/europe-west1/pipelineJobs/pipeline-test-20220309213045 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/716383847597/locations/europe-west1/pipelineJobs/pipeline-test-20220309213045 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/716383847597/locations/europe-west1/pipelineJobs/pipeline-test-20220309213045 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    }
   ],
   "source": [
    "start_pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f78759-5ac5-4305-a621-e03e49eee6c5",
   "metadata": {},
   "source": [
    "### Pipeline execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5cfa7c-2059-4cb8-b348-604ade840834",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='..data/pipeline.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb26db0",
   "metadata": {},
   "source": [
    "### List all models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9c6097",
   "metadata": {},
   "outputs": [],
   "source": [
    "DISPLAY_NAME = \"pipeline-test\"\n",
    "! gcloud ai models list --region={REGION} --filter={DISPLAY_NAME}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1de419e",
   "metadata": {},
   "source": [
    "### Schedule pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbce39c8",
   "metadata": {},
   "source": [
    "The scheduled jobs are supported by the Cloud Scheduler and Cloud Functions. \n",
    "Check that the APIs Cloud Scheduler, Cloud Functions are enabled. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4d06b5",
   "metadata": {},
   "source": [
    "### Run recurrent pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cf02fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2.google.client import AIPlatformClient\n",
    "\n",
    "api_client = AIPlatformClient(\n",
    "                project_id=PROJECT_ID,\n",
    "                region=REGION,\n",
    "                )\n",
    "\n",
    "SERVICE_ACCOUNT = (\n",
    "    \"XXXXX-compute@developer.gserviceaccount.com\" \n",
    ")\n",
    "response = api_client.create_schedule_from_job_spec(\n",
    "    enable_caching=True,\n",
    "    job_spec_path=\"ml_pipeline.json\",\n",
    "    schedule=\"0 0 * * 1\", //once per week on Monday\n",
    "    time_zone=\"Europe/Brussels\",  # change this as necessary\n",
    "    parameter_values={\"display_name\": DISPLAY_NAME},\n",
    "    pipeline_root=PIPELINE_ROOT,  # this argument is necessary if you did not specify PIPELINE_ROOT as part of the pipeline definition.\n",
    "    #service_account=SERVICE_ACCOUNT,\n",
    "    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfc56a4",
   "metadata": {},
   "source": [
    "Once the scheduled job is created, you can see it listed in the Cloud Scheduler panel in the Console."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf61af7d",
   "metadata": {},
   "source": [
    "### Test the batch prediction  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e748c28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables \n",
    "job_display_name = \"test-batch-prediction-\"\n",
    "MODEL_NAME=\"test-ridge\"\n",
    "ENDPOINT_NAME=\"test_endpoint\"\n",
    "BUCKET_URI=\"gs://BUCKET_NAME/....\"\n",
    "input_file_name=\"test.csv\"\n",
    "\n",
    "# Get model id\n",
    "MODEL_ID=!(gcloud ai models list --region=$REGION \\\n",
    "           --filter=display_name=$MODEL_NAME)\n",
    "MODEL_ID=MODEL_ID[2].split(\" \")[0]\n",
    "\n",
    "model_resource_name = f'projects/{PROJECT_ID}/locations/{REGION}/models/{MODEL_ID}'\n",
    "gcs_source= [f\"{BUCKET_URI}/{input_file_name}\"]\n",
    "gcs_destination_prefix=f\"{BUCKET_URI}/output\"\n",
    "\n",
    "def batch_prediction_job(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    model_resource_name: str,\n",
    "    job_display_name: str,\n",
    "    gcs_source: str,\n",
    "    gcs_destination_prefix: str,\n",
    "    machine_type: str,\n",
    "    starting_replica_count: int = 1, # The number of nodes for this batch prediction job. \n",
    "    max_replica_count: int = 1,    \n",
    "):   \n",
    "    aiplatform.init(project=project, location=location)\n",
    "\n",
    "    model = aiplatform.Model(model_resource_name)\n",
    "\n",
    "    batch_prediction_job = model.batch_predict(\n",
    "        job_display_name=job_display_name,\n",
    "        instances_format='csv', #json\n",
    "        gcs_source=[f\"{BUCKET_URI}/{input_file_name}\"],\n",
    "        gcs_destination_prefix=f\"{BUCKET_URI}/output\",\n",
    "        machine_type=machine_type, # must be present      \n",
    "    )\n",
    "    batch_prediction_job.wait()\n",
    "    print(batch_prediction_job.display_name)\n",
    "    print(batch_prediction_job.state)\n",
    "    return batch_prediction_job\n",
    "\n",
    "batch_prediction_job(PROJECT_ID, REGION, model_resource_name, job_display_name, gcs_source, gcs_destination_prefix, machine_type=\"n1-standard-2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e17385-50db-4af1-957c-8424251b78c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a14ebfea",
   "metadata": {},
   "source": [
    "### Send an online prediction request\n",
    "Each prediction request must be max. 1.5 MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69cd649",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENDPOINT_NAME=\"test_endpoint\"\n",
    "instance = df_raw_data.drop(columns=TARGET).sample(3).values.tolist()\n",
    "ENDPOINT_ID = !(gcloud ai endpoints list --region=$REGION \\\n",
    "              --format='value(ENDPOINT_ID)'\\\n",
    "              --filter=display_name=$ENDPOINT_NAME \\\n",
    "              --sort-by=creationTimeStamp | tail -1)\n",
    "\n",
    "ENDPOINT_ID = \"6070957850811695104\" #ENDPOINT_ID[0]\n",
    "\n",
    "def endpoint_predict(\n",
    "    project: str, location: str, instances: list, endpoint: str\n",
    "):\n",
    "    aiplatform.init(project=project, location=location)\n",
    "\n",
    "    endpoint = aiplatform.Endpoint(endpoint)\n",
    "\n",
    "    prediction = endpoint.predict(instances=instances)\n",
    "    return prediction\n",
    "\n",
    "endpoint_predict(PROJECT_ID, REGION, instance, ENDPOINT_ID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3205d2d3-d565-4547-b94f-0bdba9b7e644",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN = !(gcloud auth print-access-token)\n",
    "TOKEN = TOKEN[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77612e10-9166-41e9-b54a-ec5dd010ee3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    " \n",
    "ENDPOINT_NAME=\"test_endpoint\"\n",
    "ENDPOINT_ID = \"6070957850811695104\"\n",
    "\n",
    "\n",
    "data = {\n",
    "    'instances': df_raw_data.drop(columns=TARGET).sample(5).values.tolist()\n",
    "}\n",
    " \n",
    "def predict_(projet_id,region, endpoint_id,token, data ):\n",
    "    api = 'https://{}-aiplatform.googleapis.com/v1/projects/{}/locations/{}/endpoints/{}:predict'.format(region,projet_id, region, endpoint_id)\n",
    "    headers = {'Authorization': 'Bearer ' + token }\n",
    "    response = requests.post(api, json=data, headers=headers)\n",
    "    pred = response.json()#['predictions']\n",
    "    return pred\n",
    "   \n",
    "    \n",
    "predict_(PROJECT_ID, REGION, ENDPOINT_ID,TOKEN, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b2cf98-cef5-4414-bb7a-c58290cfcdee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "tf2-gpu.2-3.m90",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m90"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
