{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a58281c",
   "metadata": {},
   "source": [
    "# Setup use case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c7d622",
   "metadata": {},
   "source": [
    "### Enable  the APIs if they are not enabled "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce260c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''!gcloud services enable compute.googleapis.com         \\\n",
    "                       containerregistry.googleapis.com  \\\n",
    "                       aiplatform.googleapis.com  \\\n",
    "                       cloudbuild.googleapis.com \\\n",
    "                       cloudfunctions.googleapis.com\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a829d243",
   "metadata": {},
   "source": [
    "## Uncomment all needed cells if you need to install missing packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689753be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install ai platform and kfp\n",
    "#!pip3 install {USER_FLAG} google-cloud-aiplatform==1.3.0 --upgrade\n",
    "#!pip3 install {USER_FLAG} kfp --upgrade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa761c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install google_cloud_pipeline_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba102fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!gcloud auth login if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ceae6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart the kernel\n",
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c08242",
   "metadata": {},
   "source": [
    "#### Set up the global variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4377cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH=%env PATH\n",
    "%env PATH={PATH}:/home/jupyter/.local/bin\n",
    "\n",
    "CLIENT_PROJECT_NAME = 'test' \n",
    "REGION=\"europe-west1\"\n",
    "\n",
    "# Get projet name\n",
    "shell_output=!gcloud config get-value project 2> /dev/null\n",
    "PROJECT_ID=shell_output[0]\n",
    "\n",
    "# Set bucket name\n",
    "BUCKET_NAME=\"gs://\"+PROJECT_ID+\"-bucket-\" + CLIENT_PROJECT_NAME\n",
    "\n",
    "# Bucket data\n",
    "BUCKET_DATA  = BUCKET_NAME + \"/data/\"\n",
    "\n",
    "# Create bucket\n",
    "PIPELINE_ROOT = f\"{BUCKET_NAME}/pipeline_root/\"\n",
    "USER_FLAG = \"--user\"\n",
    "\n",
    "# Create bucket\n",
    "!gsutil mb -c standard -l $REGION $BUCKET_NAME\n",
    "\n",
    "\n",
    "# Model Target \n",
    "TARGET= \"target\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c4c16c",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    " * Artifact,\n",
    " * Dataset,\n",
    " * Input,\n",
    " * Model,\n",
    " * Output,\n",
    " * Metrics,\n",
    " * ClassificationMetrics\n",
    " * InputPath\n",
    " * OutputPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95845cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "import typing\n",
    "from kfp.v2 import dsl\n",
    "from kfp.v2.dsl import (Artifact,\n",
    "                        Dataset,\n",
    "                        Input,\n",
    "                        Model,\n",
    "                        Output,\n",
    "                        Metrics,\n",
    "                        ClassificationMetrics,\n",
    "                        component, \n",
    "                        OutputPath, \n",
    "                        InputPath)\n",
    "\n",
    "from kfp.v2 import compiler\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459d2b87",
   "metadata": {},
   "source": [
    "### Create pipeline\n",
    "\n",
    "We create 4 components:  \n",
    "- Load data   \n",
    "- Train a  model\n",
    "- Evaluate the model \n",
    "- Deploy the model\n",
    "\n",
    "The components have dependencies on `pandas`, `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd2d50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data to GCS \n",
    "!gsutil cp ../data/*.csv $BUCKET_DATA\n",
    "df_raw_data = pd.read_csv(\"../data/train.csv\", delimiter=\";\")\n",
    "df_raw_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fa24c0",
   "metadata": {},
   "source": [
    "### Read the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b051c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"pandas\", \"pyarrow\", \"scikit-learn==1.0.0\", \"gcsfs\"],\n",
    "    base_image=\"python:3.9\",\n",
    "    output_component_file=\"get_test_train_data.yaml\"\n",
    ")\n",
    "\n",
    "def get_data(\n",
    "    url: str,\n",
    "    dataset_train: Output[Dataset],\n",
    "    dataset_test: Output[Dataset]\n",
    "):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "   \n",
    "    train, test = pd.read_csv(url + 'training.csv', sep=';'), pd.read_csv(url + 'test.csv', sep=';')\n",
    "    train.to_csv(dataset_train.path + \".csv\" , sep=';',index=False, encoding='utf-8-sig')\n",
    "    test.to_csv(dataset_test.path + \".csv\" , sep=';',index=False, encoding='utf-8-sig')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de33e87f",
   "metadata": {},
   "source": [
    "#### Train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a50fc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install = [\"pandas\", \"pyarrow\", \"scikit-learn==1.0.0\",\"gcsfs\"], \n",
    "    base_image=\"python:3.9\",\n",
    "    output_component_file=\"train_model.yaml\"\n",
    ")\n",
    "def model_training(\n",
    "    target: str,\n",
    "    dataset:  Input[Dataset],\n",
    "    model: Output[Model], \n",
    "):\n",
    "    \n",
    "    from sklearn.linear_model import Ridge\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "\n",
    "    data = pd.read_csv(dataset.path+\".csv\", sep=';')\n",
    "    print(data.columns)\n",
    "    model_reg = Ridge(alpha= 102 ,positive = True)\n",
    "    model_reg.fit(\n",
    "        data.drop(columns=target),\n",
    "        data[target]\n",
    "    )\n",
    "    \n",
    "    model.metadata[\"framework\"] = \"Ridge\"\n",
    "    file_name = model.path + f\".pkl\"\n",
    "    with open(file_name, 'wb') as file:  \n",
    "        pickle.dump(model_reg, file)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdec3aea",
   "metadata": {},
   "source": [
    "#### Evaluate the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b304bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install = [\"pandas\", \"pyarrow\", \"scikit-learn==1.0.0\", \"gcsfs\"], \n",
    "    base_image=\"python:3.9\",\n",
    "    output_component_file=\"evaluate_model.yaml\"\n",
    ")\n",
    "def model_evaluation(\n",
    "    target : str,\n",
    "    test_set:  Input[Dataset],\n",
    "    model: Input[Model],\n",
    "    thresholds_dict_str: str,\n",
    "    kpi: Output[Metrics]\n",
    ") -> NamedTuple(\"output\", [(\"deploy\", str)]):\n",
    "\n",
    "    #from sklearn.linear_model import Ridge\n",
    "    import pandas as pd\n",
    "    import logging \n",
    "    import pickle\n",
    "    from sklearn.metrics import r2_score\n",
    "    import json\n",
    "    import typing\n",
    "\n",
    "    \n",
    "    def threshold_check(val1, val2):\n",
    "        cond = \"false\"\n",
    "        if val1 >= val2 :\n",
    "            cond = \"true\"\n",
    "        return cond\n",
    "\n",
    "    data = pd.read_csv(test_set.path+\".csv\", sep = ';')\n",
    "    file_name = model.path + \".pkl\"\n",
    "    with open(file_name, 'rb') as file:  \n",
    "        model_reg = pickle.load(file)\n",
    "    \n",
    "    y_test = data.drop(columns=target)\n",
    "    y_target=data[target]\n",
    "    y_pred = model_reg.predict(y_test)\n",
    "       \n",
    "    R2 = r2_score(y_target, y_pred)\n",
    "    thresholds_dict = json.loads(thresholds_dict_str)\n",
    "    model.metadata[\"R2\"] = float(R2)\n",
    "    kpi.log_metric(\"R2\", float(R2))\n",
    "    deploy = threshold_check(float(R2), int(thresholds_dict['R2']))\n",
    "    return (deploy,)\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139b5ad2",
   "metadata": {},
   "source": [
    "### Deploy model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae07652",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"google-cloud-aiplatform\", \"pandas\", \"pyarrow\", \"scikit-learn==1.0.0\", \"gcsfs\",  \"kfp\"],\n",
    "    base_image=\"python:3.9\",\n",
    "    output_component_file=\"pipeline_coponents.yml\"\n",
    ")\n",
    "def deploy_model(\n",
    "    model: Input[Model],\n",
    "    project: str,\n",
    "    region: str,\n",
    "    serving_container_image_uri : str, \n",
    "    vertex_endpoint: Output[Artifact],\n",
    "    vertex_model: Output[Model]\n",
    "):\n",
    "    from google.cloud import aiplatform\n",
    "    aiplatform.init(project=project, location=region)\n",
    "\n",
    "    DISPLAY_NAME  = \"test\"\n",
    "    MODEL_NAME = \"test-ridge\"\n",
    "    ENDPOINT_NAME = \"test_endpoint\"\n",
    "    \n",
    "    def create_endpoint():\n",
    "        endpoints = aiplatform.Endpoint.list(\n",
    "        filter='display_name=\"{}\"'.format(ENDPOINT_NAME),\n",
    "        order_by='create_time desc',\n",
    "        project=project, \n",
    "        location=region,\n",
    "        )\n",
    "        if len(endpoints) > 0:\n",
    "            endpoint = endpoints[0]  # most recently created\n",
    "        else:\n",
    "            endpoint = aiplatform.Endpoint.create(\n",
    "            display_name=ENDPOINT_NAME, project=project, location=region\n",
    "        )\n",
    "    endpoint = create_endpoint()   \n",
    "    \n",
    "    \n",
    "    #Import a model programmatically\n",
    "    model_upload = aiplatform.Model.upload(\n",
    "        display_name = DISPLAY_NAME, \n",
    "        artifact_uri = model.uri[:-5], #.replace(\"model\", \"\"),\n",
    "        serving_container_image_uri =  serving_container_image_uri,\n",
    "        serving_container_health_route=f\"/v1/models/{MODEL_NAME}\",\n",
    "        serving_container_predict_route=f\"/v1/models/{MODEL_NAME}:predict\",\n",
    "        serving_container_environment_variables={\n",
    "        \"MODEL_NAME\": MODEL_NAME,\n",
    "    },       \n",
    "    )\n",
    "    model_deploy = model_upload.deploy(\n",
    "        machine_type=\"n1-standard-4\", \n",
    "        endpoint=endpoint,\n",
    "        traffic_split={\"0\": 100},\n",
    "        deployed_model_display_name=DISPLAY_NAME,\n",
    "    )\n",
    "\n",
    "    # Save data to the output params\n",
    "    vertex_model.uri = model_deploy.resource_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7902a0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP =datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "DISPLAY_NAME = 'pipeline-test-job{}'.format(TIMESTAMP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98bd8c3",
   "metadata": {},
   "source": [
    "#### Create the Pipeline\n",
    "\n",
    "Once you have created all the needed components define the pipeline and then compile it into a `.json` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d5f227",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    # Default pipeline root. You can override it when submitting the pipeline.\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    # A name for the pipeline. Use to determine the pipeline Context.\n",
    "    name=\"pipeline-test\",\n",
    "    \n",
    ")\n",
    "def pipeline(\n",
    "    url: str = BUCKET_NAME + \"/data/\",\n",
    "    target : str = TARGET,\n",
    "    project: str = PROJECT_ID,\n",
    "    region: str = REGION, \n",
    "    display_name: str = DISPLAY_NAME,\n",
    "    api_endpoint: str = REGION+\"-aiplatform.googleapis.com\",\n",
    "    thresholds_dict_str: str = '{\"R2\":0.45}',\n",
    "    serving_container_image_uri: str = \"europe-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.0-24:latest\"\n",
    "    ):\n",
    "    \n",
    "    data_op = get_data(url)\n",
    "    \n",
    "    train_model_op = model_training(target, data_op.outputs[\"dataset_train\"])\n",
    "    \n",
    "    model_evaluation_op = model_evaluation(\n",
    "        target,\n",
    "        test_set=data_op.outputs[\"dataset_test\"],\n",
    "        model=train_model_op.outputs[\"model\"],\n",
    "        thresholds_dict_str = thresholds_dict_str, # I deploy the model only if the model performance is above the threshold\n",
    "    )\n",
    "    \n",
    "    with dsl.Condition(\n",
    "        model_evaluation_op.outputs[\"deploy\"]==\"true\",\n",
    "        name=\"deploy-model\",\n",
    "    ):\n",
    "           \n",
    "        deploy_model_op = deploy_model(\n",
    "        model=train_model_op.outputs['model'],\n",
    "        project=project,\n",
    "        region=region, \n",
    "        serving_container_image_uri = serving_container_image_uri,\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba00f47",
   "metadata": {},
   "source": [
    "### Compile and run the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68ec6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(pipeline_func=pipeline,\n",
    "        package_path='ml_pipeline.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68be6e6",
   "metadata": {},
   "source": [
    "The pipeline compilation generates the **ml_winequality.json** job spec file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e47a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create a run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24bac81",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_pipeline = pipeline_jobs.PipelineJob(\n",
    "    display_name=\"pipeline-test\",\n",
    "    template_path=\"ml_pipeline.json\",\n",
    "    enable_caching=False,\n",
    "    location=REGION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f7bd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb26db0",
   "metadata": {},
   "source": [
    "### List all models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9c6097",
   "metadata": {},
   "outputs": [],
   "source": [
    "DISPLAY_NAME = \"pipeline-test\"\n",
    "! gcloud ai models list --region={REGION} --filter={DISPLAY_NAME}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1de419e",
   "metadata": {},
   "source": [
    "### Schedule pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbce39c8",
   "metadata": {},
   "source": [
    "The scheduled jobs are supported by the Cloud Scheduler and Cloud Functions. \n",
    "Check that the APIs Cloud Scheduler, Cloud Functions are enabled. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4d06b5",
   "metadata": {},
   "source": [
    "### Run recurrent pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cf02fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2.google.client import AIPlatformClient\n",
    "\n",
    "api_client = AIPlatformClient(\n",
    "                project_id=PROJECT_ID,\n",
    "                region=REGION,\n",
    "                )\n",
    "\n",
    "SERVICE_ACCOUNT = (\n",
    "    \"XXXXX-compute@developer.gserviceaccount.com\" \n",
    ")\n",
    "response = api_client.create_schedule_from_job_spec(\n",
    "    enable_caching=True,\n",
    "    job_spec_path=\"ml_pipeline.json\",\n",
    "    schedule=\"0 0 * * 1\", //once per week on Monday\n",
    "    time_zone=\"Europe/Brussels\",  # change this as necessary\n",
    "    parameter_values={\"display_name\": DISPLAY_NAME},\n",
    "    pipeline_root=PIPELINE_ROOT,  # this argument is necessary if you did not specify PIPELINE_ROOT as part of the pipeline definition.\n",
    "    #service_account=SERVICE_ACCOUNT,\n",
    "    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfc56a4",
   "metadata": {},
   "source": [
    "Once the scheduled job is created, you can see it listed in the Cloud Scheduler panel in the Console."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf61af7d",
   "metadata": {},
   "source": [
    "### Test the batch prediction  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e748c28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables \n",
    "job_display_name = \"test-batch-prediction-\"\n",
    "MODEL_NAME=\"winequality\"\n",
    "ENDPOINT_NAME=\"winequality_endpoint\"\n",
    "BUCKET_URI=\"gs://BUCKET_NAME/....\"\n",
    "input_file_name=\"test.csv\"\n",
    "\n",
    "# Get model id\n",
    "MODEL_ID=!(gcloud ai models list --region=$REGION \\\n",
    "           --filter=display_name=$MODEL_NAME)\n",
    "MODEL_ID=MODEL_ID[2].split(\" \")[0]\n",
    "\n",
    "model_resource_name = f'projects/{PROJECT_ID}/locations/{REGION}/models/{MODEL_ID}'\n",
    "gcs_source= [f\"{BUCKET_URI}/{input_file_name}\"]\n",
    "gcs_destination_prefix=f\"{BUCKET_URI}/output\"\n",
    "\n",
    "def batch_prediction_job(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    model_resource_name: str,\n",
    "    job_display_name: str,\n",
    "    gcs_source: str,\n",
    "    gcs_destination_prefix: str,\n",
    "    machine_type: str,\n",
    "    starting_replica_count: int = 1, # The number of nodes for this batch prediction job. \n",
    "    max_replica_count: int = 1,    \n",
    "):   \n",
    "    aiplatform.init(project=project, location=location)\n",
    "\n",
    "    model = aiplatform.Model(model_resource_name)\n",
    "\n",
    "    batch_prediction_job = model.batch_predict(\n",
    "        job_display_name=job_display_name,\n",
    "        instances_format='csv', #json\n",
    "        gcs_source=[f\"{BUCKET_URI}/{input_file_name}\"],\n",
    "        gcs_destination_prefix=f\"{BUCKET_URI}/output\",\n",
    "        machine_type=machine_type, # must be present      \n",
    "    )\n",
    "    batch_prediction_job.wait()\n",
    "    print(batch_prediction_job.display_name)\n",
    "    print(batch_prediction_job.state)\n",
    "    return batch_prediction_job\n",
    "\n",
    "batch_prediction_job(PROJECT_ID, REGION, model_resource_name, job_display_name, gcs_source, gcs_destination_prefix, machine_type=\"n1-standard-2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14ebfea",
   "metadata": {},
   "source": [
    "### Send an online prediction request\n",
    "Each prediction request must be max. 1.5 MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69cd649",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENDPOINT_NAME=\"test_endpoint\"\n",
    "instance = df_raw_data.drop(columns=TARGET).sample(3).values.tolist()\n",
    "ENDPOINT_ID = !(gcloud ai endpoints list --region=$REGION \\\n",
    "              --format='value(ENDPOINT_ID)'\\\n",
    "              --filter=display_name=$ENDPOINT_NAME \\\n",
    "              --sort-by=creationTimeStamp | tail -1)\n",
    "\n",
    "ENDPOINT_ID = \"6070957850811695104\" #ENDPOINT_ID[0]\n",
    "\n",
    "def endpoint_predict(\n",
    "    project: str, location: str, instances: list, endpoint: str\n",
    "):\n",
    "    aiplatform.init(project=project, location=location)\n",
    "\n",
    "    endpoint = aiplatform.Endpoint(endpoint)\n",
    "\n",
    "    prediction = endpoint.predict(instances=instances)\n",
    "    return prediction\n",
    "\n",
    "endpoint_predict(PROJECT_ID, REGION, instance, ENDPOINT_ID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3205d2d3-d565-4547-b94f-0bdba9b7e644",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN = !(gcloud auth print-access-token)\n",
    "TOKEN = TOKEN[0]\n",
    "TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77612e10-9166-41e9-b54a-ec5dd010ee3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    " \n",
    "ENDPOINT_NAME=\"test_endpoint\"\n",
    "ENDPOINT_ID = \"6070957850811695104\"\n",
    "\n",
    "\n",
    "data = {\n",
    "    'instances': df_raw_data.drop(columns=TARGET).sample(5).values.tolist()\n",
    "}\n",
    " \n",
    "def predict_(projet_id,region, endpoint_id,token, data ):\n",
    "    api = 'https://{}-aiplatform.googleapis.com/v1/projects/{}/locations/{}/endpoints/{}:predict'.format(region,projet_id, region, endpoint_id)\n",
    "    headers = {'Authorization': 'Bearer ' + token }\n",
    "    response = requests.post(api, json=data, headers=headers)\n",
    "    pred = response.json()#['predictions']\n",
    "    return pred\n",
    "   \n",
    "    \n",
    "predict_(PROJECT_ID, REGION, ENDPOINT_ID,TOKEN, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b2cf98-cef5-4414-bb7a-c58290cfcdee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "tf2-gpu.2-3.m90",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m90"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
